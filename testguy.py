import json
import os
import pandas as pd
from openai import OpenAI
from tqdm import tqdm

# --- AYARLAR ---
# LM Studio'da tÃ¼m ayarlarÄ± yaptÄ±ÄŸÄ±mÄ±z iÃ§in burasÄ± sadeleÅŸti
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

INPUT_FILE = "human-labeled-sample-1495.csv"
OUTPUT_FILE = "gemma-3-12b-it_sonuclar.json"
SUTUN_ADI = "text"  # Senin CSV sÃ¼tun baÅŸlÄ±ÄŸÄ±n

# --- 1. VERÄ° OKUMA ---
try:
    df = pd.read_csv(INPUT_FILE, sep=";")


    # SÃ¼tun adÄ± kontrolÃ¼ (Garanti olsun)
    if SUTUN_ADI not in df.columns:
        print(f"HATA: '{SUTUN_ADI}' sÃ¼tunu bulunamadÄ±! Mevcut sÃ¼tunlar: {list(df.columns)}")
        # Belki 'Text' bÃ¼yÃ¼k harflidir diye alternatif kontrol
        if "Text" in df.columns:
            SUTUN_ADI = "Text"
            print("-> 'Text' sÃ¼tunu bulundu, onunla devam ediliyor.")
        else:
            exit()

    # EÄŸer dosya Ã§ok bÃ¼yÃ¼kse ve sample alÄ±yorsan, random_state SABÄ°T olmalÄ± ki
    # programÄ± yeniden baÅŸlattÄ±ÄŸÄ±nda yine aynÄ± satÄ±rlar gelsin.
    if len(df) > 3000:
        df = df.sample(n=3000, random_state=42).reset_index(drop=True)

    tum_yorumlar = df[SUTUN_ADI].astype(str).tolist()
    print(f"Hedef: Toplam {len(tum_yorumlar)} yorum analiz edilecek.")

except Exception as e:
    print(f"CSV HatasÄ±: {e}")
    exit()

# --- 2. KALDIÄIMIZ YERÄ° BULMA (CHECKPOINT) ---
mevcut_sonuclar = []

if os.path.exists(OUTPUT_FILE):
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            mevcut_sonuclar = json.load(f)
        print(f"âœ… Ã–nceki kayÄ±t bulundu! {len(mevcut_sonuclar)} tanesi zaten yapÄ±lmÄ±ÅŸ.")
    except:
        print("âš ï¸ KayÄ±t dosyasÄ± bozuk veya boÅŸ, sÄ±fÄ±rdan baÅŸlanÄ±yor.")
        mevcut_sonuclar = []

baslangic_index = len(mevcut_sonuclar)

# EÄŸer hepsi bitmiÅŸse boÅŸuna yorma
if baslangic_index >= len(tum_yorumlar):
    print("ğŸ‰ TÃ¼m analizler zaten tamamlanmÄ±ÅŸ! Dosya hazÄ±r.")
    exit()

print(f"ğŸš€ {baslangic_index + 1}. yorumdan devam ediliyor...")

# --- 3. DÃ–NGÃœ VE KAYDETME ---
# tqdm'e initial parametresini veriyoruz ki bar doÄŸru yerden baÅŸlasÄ±n
for i in tqdm(range(baslangic_index, len(tum_yorumlar)), initial=baslangic_index, total=len(tum_yorumlar)):
    yorum = tum_yorumlar[i]

    try:
        # System Prompt'u LM Studio arayÃ¼zÃ¼nden ayarladÄ±k, burasÄ± boÅŸ kalabilir
        # Veya garanti olsun diye basit bir reminder atabiliriz.
        response = client.chat.completions.create(
            model="local-model",
            messages=[
                {"role": "user", "content": f'Yorum: "{yorum}"'}
            ],
            temperature=0.1,
        )

        raw_content = response.choices[0].message.content.strip()

        # Structured Output kullansan bile bazen temizlik gerekebilir
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        parsed_json = json.loads(clean_content)

        mevcut_sonuclar.append({
            "index": i,
            "yorum": yorum,
            "analiz": parsed_json
        })

    except Exception as e:
        # Hata olursa da kaydet, durmasÄ±n
        mevcut_sonuclar.append({
            "index": i,
            "yorum": yorum,
            "error": str(e)
        })

    # --- KRÄ°TÄ°K KISIM: HER 100 ADETTE BÄ°R KAYDET ---
    if (i + 1) % 100 == 0:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(mevcut_sonuclar, f, ensure_ascii=False, indent=4)
        # Tqdm barÄ±nÄ± bozmamak iÃ§in print yapmÄ±yoruz, arkada kaydetti.

# --- BÄ°TÄ°ÅTE SON KAYIT ---
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(mevcut_sonuclar, f, ensure_ascii=False, indent=4)

print(f"\nğŸ Ä°ÅLEM TAMAMLANDI! Toplam {len(mevcut_sonuclar)} sonuÃ§ kaydedildi.")